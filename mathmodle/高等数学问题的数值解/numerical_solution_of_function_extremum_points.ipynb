{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c39bdc31",
   "metadata": {},
   "source": [
    "# 函数极值点的数值解\n",
    "\n",
    "### 通过SciPy库函数求极值点\n",
    "### 1. 无约束优化（标量函数）\n",
    "scipy.optimize.minimize_scalar()\n",
    "    result = minimize_scalar(fun, bracket=None, bounds=None, method='brent', tol=None, options=None)\n",
    "#### 关键参数解释\n",
    "        fun (必需):\n",
    "            你要最小化的函数。它必须是一个接受单个标量（数字）作为输入，并返回一个标量（函数值）的 Python 函数。\n",
    "        bracket (可选):\n",
    "            一个包含 2 个或 3 个数的元组或列表，如 (a, b) 或 (a, b, c)。它提供了一个包含最小值的初始区间（括号）。\n",
    "            对于 3 个数的括号 (a, b, c)，要求 f(b) < f(a) 且 f(b) < f(c)，即 b 点的函数值比两端点都小，这有助于算法更快收敛。\n",
    "            如果不提供，算法可能会尝试自己确定一个初始区间。\n",
    "        bounds (可选):\n",
    "            一个包含 2 个数的元组或列表，如 (a, b)，表示搜索的边界约束。算法只会在区间 [a, b] 内寻找最小值。\n",
    "            当 method='bounded' 时，这个参数是必需的。\n",
    "        method (可选, 默认 'brent'):\n",
    "            指定使用的优化算法。常用选项有：\n",
    "            'brent'：默认方法。结合了黄金分割搜索和抛物线插值，效率高，通常用于无约束最小化。如果提供了 bracket，它会使用这个区间。\n",
    "            'bounded'：仅在有界区间 [a, b] 内搜索最小值。必须提供 bounds 参数。它使用有界版本的 Brent 方法。\n",
    "            'golden'：黄金分割搜索法，较慢但稳定，不依赖导数。\n",
    "        tol (可选):\n",
    "            优化的容忍度（容差），控制算法的终止精度。值越小，结果越精确，但可能需要更多计算时间。如果 method='brent' 或 'golden'，这个容差是针对 x 的；如果 method='bounded'，则是针对 x 的相对和绝对容差。\n",
    "        options (可选):\n",
    "            一个字典，用于传递特定于方法的选项，例如：\n",
    "                maxiter：最大迭代次数。\n",
    "                disp：布尔值，是否打印收敛信息（仅当 method='brent' 或 'golden' 时有效）。\n",
    "#### 主要功能\n",
    "        目标：在给定的区间内，寻找一元函数 f(x) 的局部或全局最小值点 x_min。\n",
    "        返回：一个包含优化结果的 OptimizeResult 对象，通常包含：\n",
    "            x：找到的最小值点（自变量的值）。\n",
    "            fun：在 x 处的函数值（即最小值 f(x_min)）。\n",
    "            success：布尔值，表示优化是否成功。\n",
    "            message：描述优化过程的信息或错误信息。\n",
    "            nfev：函数被调用的次数。\n",
    "            nit：迭代次数。\n",
    "            其他与具体算法相关的字段。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0e01251",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用 Brent 方法:\n",
      "最小值点 x = -1.0000000000000002\n",
      "最小值 f(x) = 0.0\n",
      "成功? True\n",
      "\n",
      "在区间 [-5, 0] 内使用 Bounded 方法:\n",
      "最小值点 x = -0.9999999999999998\n",
      "最小值 f(x) = 0.0\n",
      "\n",
      "提供初始 bracket (-3, -1, 1):\n",
      "最小值点 x = -1\n",
      "最小值 f(x) = 0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize_scalar\n",
    "\n",
    "# 定义一个要最小化的函数 f(x) = x^2 + 2x + 1\n",
    "def objective_function(x):\n",
    "    return x**2 + 2*x + 1\n",
    "\n",
    "# 示例 1: 使用默认的 'brent' 方法 (无约束)\n",
    "result1 = minimize_scalar(objective_function)\n",
    "print(\"使用 Brent 方法:\")\n",
    "print(f\"最小值点 x = {result1.x}\")\n",
    "print(f\"最小值 f(x) = {result1.fun}\")\n",
    "print(f\"成功? {result1.success}\")\n",
    "# 输出: x ≈ -1.0, f(x) ≈ 0.0\n",
    "\n",
    "# 示例 2: 使用 'bounded' 方法，在区间 [-5, 0] 内搜索\n",
    "result2 = minimize_scalar(objective_function, bounds=(-5, 0), method='bounded')\n",
    "print(\"\\n在区间 [-5, 0] 内使用 Bounded 方法:\")\n",
    "print(f\"最小值点 x = {result2.x}\")\n",
    "print(f\"最小值 f(x) = {result2.fun}\")\n",
    "# 输出: 同样会找到 x ≈ -1.0 (因为它在区间内)\n",
    "\n",
    "# 示例 3: 提供初始 bracket\n",
    "result3 = minimize_scalar(objective_function, bracket=(-3, -1, 1)) # f(-1) < f(-3) 且 f(-1) < f(1)\n",
    "print(\"\\n提供初始 bracket (-3, -1, 1):\")\n",
    "print(f\"最小值点 x = {result3.x}\")\n",
    "print(f\"最小值 f(x) = {result3.fun}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45152407",
   "metadata": {},
   "source": [
    "### 2. 无约束优化（多变量函数）\n",
    "result = minimize(fun, x0, args=(), method=None, jac=None, hess=None, hessp=None, bounds=None, constraints=(), tol=None, callback=None, options=None)\n",
    "#### 参数说明\n",
    "    fun: 目标函数，需要最小化的函数。它应该接受一个参数（表示变量的数组）并返回一个标量。\n",
    "    x0: 初始猜测值，一个代表初始点的数组。\n",
    "    args: 可选参数，传递给目标函数和其导数的额外参数。\n",
    "    method: 使用的优化算法。默认为 BFGS，但也有其他选项如 'Nelder-Mead', 'Powell', 'CG', 'Newton-CG', 'L-BFGS-B', 'TNC', 'COBYLA', 'SLSQP', 'trust-constr' 等等。\n",
    "    jac: 目标函数的雅可比矩阵（梯度）。可以是一个函数，也可以是布尔值（如果设置为 True，则使用 fun 的输出自动计算）。\n",
    "    bounds: 参数的边界限制，适用于那些要求参数在特定范围内的情况。这通常是一个 Bounds 对象列表。\n",
    "    constraints: 约束条件，可以是等式或不等式约束。约束以字典形式提供，包含类型 ('eq' 或 'ineq') 和函数表达式。\n",
    "    tol: 终止容差，取决于所选择的方法。\n",
    "    options: 一个字典，用于指定方法特定的选项，如最大迭代次数(maxiter)、显示级别(disp)等。\n",
    "#### 返回值\n",
    "    minimize() 函数返回一个 OptimizeResult 对象，该对象包含了优化的结果信息，包括但不限于：\n",
    "    x: 最优解（即找到的最小值点）。\n",
    "    success: 布尔值，指示优化是否成功。\n",
    "    message: 描述终止原因的字符串。\n",
    "    fun: 最优解处的目标函数值。\n",
    "    jac: 最优解处的雅可比估计值（如果适用）。\n",
    "    nit: 执行的迭代次数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99c10ffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最优解: [0.99999997 2.5       ]\n",
      "最优值: 1.0075886140706989e-15\n",
      "成功: True\n",
      "消息: Optimization terminated successfully.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "def objective_function(x):\n",
    "    return (x[0] - 1)**2 + (x[1] - 2.5)**2\n",
    "# 初始猜测值\n",
    "x0 = [2, 0]\n",
    "# 调用 minimize 函数\n",
    "res = minimize(objective_function, x0)\n",
    "print(\"最优解:\", res.x)\n",
    "print(\"最优值:\", res.fun)\n",
    "print(\"成功:\", res.success)\n",
    "print(\"消息:\", res.message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f0dad0",
   "metadata": {},
   "source": [
    "### 3. 全局优化\n",
    "是 SciPy 优化模块 (scipy.optimize) 中的一个函数，用于执行全局优化，特别适用于寻找多变量函数在给定边界约束下的全局最小值。\n",
    "它实现的是一种名为差分进化 (Differential Evolution, DE) 的进化算法。这种算法不依赖于目标函数的梯度信息，因此非常适合优化那些不可导、不连续、有噪声、或者存在多个局部极小值的复杂函数。\n",
    "#### 核心思想\n",
    "    差分进化是一种基于种群的随机搜索算法：\n",
    "        初始化：在指定的搜索空间（由 bounds 定义）内随机生成一组初始解（称为“个体”或“向量”），这些个体构成一个“种群”。\n",
    "        变异 (Mutation)：对于种群中的每个个体，随机选择另外三个不同的个体，利用它们的差异（向量差）来生成一个“变异向量”。这个过程引入了探索新区域的能力。\n",
    "        交叉 (Crossover)：将当前个体（“父代”）与变异向量进行交叉（混合），生成一个“试验向量”。这增加了种群的多样性。\n",
    "        选择 (Selection)：比较试验向量和原始父代个体的函数值。如果试验向量的函数值更小（更优），则用它替换父代个体进入下一代种群；否则，父代个体保留。\n",
    "        迭代：重复变异、交叉和选择步骤，直到满足终止条件（如达到最大迭代次数 maxiter 或函数值变化小于容差 tol）。\n",
    "    通过这种机制，算法在搜索空间中探索（Exploration）和利用（Exploitation）之间进行平衡，逐步逼近全局最优解。\n",
    "\n",
    "result = differential_evolution(func, bounds, args=(), strategy='best1bin', maxiter=None, popsize=15, tol=0.01, mutation=(0.5, 1), recombination=0.7, seed=None, callback=None, disp=False, polish=True, init='latinhypercube', atol=0, updating='immediate', workers=1, constraints=(), x0=None)\n",
    "#### 关键参数解释\n",
    "    func (必需):\n",
    "        要最小化的目标函数。它必须接受一个包含 D 个元素的数组（x，其中 D 是变量个数）作为输入，并返回一个标量（函数值 f(x)）。\n",
    "    bounds (必需):\n",
    "        一个包含 D 个元组的序列（如列表或元组），每个元组定义了对应变量的 (最小值, 最大值) 边界。例如，bounds = [(0, 1), (-2, 2)] 表示第一个变量 x[0] 在 [0, 1] 之间，第二个变量 x[1] 在 [-2, 2] 之间。这是定义搜索空间的关键。\n",
    "    strategy (可选, 默认 'best1bin'):\n",
    "        差分进化变异策略。常用选项：\n",
    "            'best1bin': mutant = best + mutation * (rand1 - rand2) (二进制交叉)\n",
    "            'best1exp': 同上，但使用指数交叉。\n",
    "            'rand1bin': mutant = rand0 + mutation * (rand1 - rand2) (二进制交叉)\n",
    "            'randtobest1bin': 结合随机和最优的策略。\n",
    "            'currenttobest1bin': 基于当前个体和最优个体的策略。\n",
    "        不同策略在探索和利用能力上有所侧重。\n",
    "    maxiter (可选, 默认 1000):\n",
    "        最大迭代次数。算法最多运行这么多代。\n",
    "    popsize (可选, 默认 15):\n",
    "        种群大小，表示每一代中有多少个个体。通常设置为 popsize * len(bounds) 个个体。值越大，探索能力越强，但计算成本越高。默认 15 意味着种群大小约为 15 * D。\n",
    "    tol (可选, 默认 0.01):\n",
    "        相对容差，用于判断收敛。当种群中所有个体的函数值的标准差小于 tol * |f_opt|（f_opt 是当前最优值）时，算法可能提前终止。注意：tol 在 workers > 1 时被忽略。\n",
    "    mutation (可选, 默认 (0.5, 1)):\n",
    "        变异因子 F。如果是单个数，则在整个运行过程中使用固定的 F。如果是元组 (low, high)，则 F 在 [low, high] 区间内随机变化，有助于跳出局部最优。F 控制变异向量的步长。\n",
    "    recombination (可选, 默认 0.7):\n",
    "        交叉常数 CR，范围 [0, 1]。它控制试验向量从变异向量继承分量的概率。CR 越高，交叉越频繁。\n",
    "    polish (可选, 默认 True):\n",
    "        布尔值。如果为 True，在差分进化算法结束后，会使用 scipy.optimize.minimize() 函数（默认方法为 L-BFGS-B）以找到的全局最优解为起点进行局部优化（“打磨”），以进一步提高解的精度。\n",
    "    init (可选, 默认 'latinhypercube'):\n",
    "        种群初始化方法。\n",
    "            'latinhypercube': 拉丁超立方采样，能更均匀地覆盖搜索空间，通常比纯随机更好。\n",
    "            'random': 在边界内均匀随机采样。\n",
    "        也可以传入一个形状为 (M, D) 的数组，其中 M 至少为 popsize * D。\n",
    "    updating (可选, 默认 'immediate'):\n",
    "        'immediate': 生成一个试验向量后立即进行选择，成功则更新种群。\n",
    "        'deferred': 为整个种群生成所有试验向量后，再统一进行选择和更新（更适合并行 workers > 1）。\n",
    "    workers (可选, 默认 1):\n",
    "        用于并行评估目标函数的进程数。如果 workers > 1，则使用 multiprocessing 并行计算种群中所有个体的函数值，可以显著加速（尤其当 func 计算耗时）。workers=-1 表示使用所有可用的CPU核心。注意：func 必须能被 pickle（通常意味着不能是嵌套函数或 lambda 函数，除非在特定环境下）。\n",
    "    constraints (可选):\n",
    "        与 minimize() 类似，可以添加非线性约束（等式或不等式）。约束以字典列表形式提供。\n",
    "    x0 (可选, SciPy 1.2.0+):\n",
    "        一个初始猜测解。如果提供，它会被包含在初始种群中（替换一个随机个体），有助于引导搜索。\n",
    "#### 返回值\n",
    "    返回一个 OptimizeResult 对象，包含优化结果：\n",
    "        x: 找到的最优解（自变量向量）。\n",
    "        fun: 在 x 处的最优函数值（全局最小值估计）。\n",
    "        success: 布尔值，表示优化是否成功。\n",
    "        message: 描述终止原因的字符串。\n",
    "        nfev: 目标函数被调用的总次数。\n",
    "        nit: 实际执行的迭代次数。\n",
    "        jac: 未定义（因为DE不使用梯度）。\n",
    "        nit: 迭代次数。\n",
    "        final_pop: （仅当 return_all=True 时存在，但此参数已弃用）最终种群。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dbe13242",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "differential_evolution step 1: f(x)= 6.8914624659612205\n",
      "differential_evolution step 2: f(x)= 1.3976479290463217\n",
      "differential_evolution step 3: f(x)= 1.3976479290463217\n",
      "differential_evolution step 4: f(x)= 1.3976479290463217\n",
      "differential_evolution step 5: f(x)= 1.3976479290463217\n",
      "differential_evolution step 6: f(x)= 1.3976479290463217\n",
      "differential_evolution step 7: f(x)= 1.3536449204796526\n",
      "differential_evolution step 8: f(x)= 1.3536449204796526\n",
      "differential_evolution step 9: f(x)= 1.0514695071833131\n",
      "differential_evolution step 10: f(x)= 1.0514695071833131\n",
      "differential_evolution step 11: f(x)= 1.0514695071833131\n",
      "differential_evolution step 12: f(x)= 1.0240190916809198\n",
      "differential_evolution step 13: f(x)= 1.0240190916809198\n",
      "differential_evolution step 14: f(x)= 0.04875172731551913\n",
      "differential_evolution step 15: f(x)= 0.04065864424849153\n",
      "differential_evolution step 16: f(x)= 0.037423712815286336\n",
      "differential_evolution step 17: f(x)= 0.0204325491284294\n",
      "differential_evolution step 18: f(x)= 0.007613186104734382\n",
      "differential_evolution step 19: f(x)= 0.0004050821172327801\n",
      "differential_evolution step 20: f(x)= 0.0004050821172327801\n",
      "differential_evolution step 21: f(x)= 4.4550336752990916e-05\n",
      "differential_evolution step 22: f(x)= 4.4550336752990916e-05\n",
      "differential_evolution step 23: f(x)= 2.4541965132129917e-05\n",
      "differential_evolution step 24: f(x)= 1.6174882127018009e-06\n",
      "differential_evolution step 25: f(x)= 1.5941948561248864e-07\n",
      "differential_evolution step 26: f(x)= 1.5941948561248864e-07\n",
      "differential_evolution step 27: f(x)= 1.5941948561248864e-07\n",
      "differential_evolution step 28: f(x)= 1.206103590334351e-07\n",
      "differential_evolution step 29: f(x)= 2.1144586526133935e-08\n",
      "differential_evolution step 30: f(x)= 2.1144586526133935e-08\n",
      "differential_evolution step 31: f(x)= 9.256559962977917e-09\n",
      "differential_evolution step 32: f(x)= 3.3153781942019123e-09\n",
      "differential_evolution step 33: f(x)= 3.3153781942019123e-09\n",
      "differential_evolution step 34: f(x)= 1.412203687323199e-10\n",
      "differential_evolution step 35: f(x)= 5.2366999625519384e-11\n",
      "differential_evolution step 36: f(x)= 1.6626700016786344e-11\n",
      "differential_evolution step 37: f(x)= 8.86402062860725e-12\n",
      "differential_evolution step 38: f(x)= 8.86402062860725e-12\n",
      "differential_evolution step 39: f(x)= 2.842170943040401e-14\n",
      "differential_evolution step 40: f(x)= 2.842170943040401e-14\n",
      "differential_evolution step 41: f(x)= 2.842170943040401e-14\n",
      "differential_evolution step 42: f(x)= 2.842170943040401e-14\n",
      "differential_evolution step 43: f(x)= 2.842170943040401e-14\n",
      "differential_evolution step 44: f(x)= 2.842170943040401e-14\n",
      "differential_evolution step 45: f(x)= 1.4210854715202004e-14\n",
      "differential_evolution step 46: f(x)= 7.105427357601002e-15\n",
      "differential_evolution step 47: f(x)= 7.105427357601002e-15\n",
      "differential_evolution step 48: f(x)= 7.105427357601002e-15\n",
      "differential_evolution step 49: f(x)= 0.0\n",
      "differential_evolution step 50: f(x)= 0.0\n",
      "differential_evolution step 51: f(x)= 0.0\n",
      "differential_evolution step 52: f(x)= 0.0\n",
      "differential_evolution step 53: f(x)= 0.0\n",
      "differential_evolution step 54: f(x)= 0.0\n",
      "differential_evolution step 55: f(x)= 0.0\n",
      "differential_evolution step 56: f(x)= 0.0\n",
      "differential_evolution step 57: f(x)= 0.0\n",
      "Polishing solution with 'L-BFGS-B'\n",
      "最优解 x: [ 1.30534318e-09 -1.03853438e-09]\n",
      "最优值 f(x): 0.0\n",
      "成功: True\n",
      "消息: Optimization terminated successfully.\n",
      "函数调用次数: 1743\n",
      "迭代次数: 57\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import differential_evolution\n",
    "\n",
    "# 定义一个具有多个局部最小值的复杂函数 (Rastrigin 函数，全局最小值在 (0,0))\n",
    "def rastrigin(x):\n",
    "    A = 10\n",
    "    return A * len(x) + sum([(xi**2 - A * np.cos(2 * np.pi * xi)) for xi in x])\n",
    "# 定义搜索边界\n",
    "bounds = [(-5.12, 5.12), (-5.12, 5.12)] # 二维\n",
    "# 调用 differential_evolution\n",
    "result = differential_evolution(\n",
    "    rastrigin,\n",
    "    bounds,\n",
    "    strategy='best1bin',\n",
    "    maxiter=1000,\n",
    "    popsize=15,\n",
    "    tol=0.01,\n",
    "    mutation=(0.5, 1),\n",
    "    recombination=0.7,\n",
    "    seed=42, # 为了可重现性\n",
    "    disp=True, # 显示进度\n",
    "    polish=True, # 最后进行局部优化\n",
    "    init='latinhypercube',\n",
    "    workers=1 # 单进程运行\n",
    ")\n",
    "print(\"最优解 x:\", result.x)\n",
    "print(\"最优值 f(x):\", result.fun)\n",
    "print(\"成功:\", result.success)\n",
    "print(\"消息:\", result.message)\n",
    "print(\"函数调用次数:\", result.nfev)\n",
    "print(\"迭代次数:\", result.nit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f86831",
   "metadata": {},
   "source": [
    "# 1.一元函数的极值点\n",
    "    scipy.optimize.fminbound() 是 SciPy 优化模块 (scipy.optimize) 中的一个函数，\n",
    "    用于在给定的有限区间内寻找单变量（一元）函数的最小值。\n",
    "    你可以把它理解为 minimize_scalar(method='bounded') 的一个便捷封装，专门用于解决“在区间 [x1, x2] 内，找到让函数 f(x) 最小的那个 x”这类问题。\n",
    "## 主要特点\n",
    "    单变量函数：只能优化只有一个自变量 x 的函数 f(x)。\n",
    "    有界区间：必须明确指定搜索的下限 x1 和上限 x2。算法只在这个闭区间 [x1, x2] 内寻找最小值。\n",
    "    标量最小化：目标是找到最小值点 x_min 和对应的最小函数值 f(x_min)。\n",
    "    基于 Brent 方法：内部使用了健壮的 Brent 算法（结合了黄金分割搜索和抛物线插值），效率较高。\n",
    "    不保证全局最优：虽然在指定区间内搜索，但对于有多个极小值的函数，它找到的是区间内的一个局部最小值，不一定是全局最小值（尤其是在区间内存在多个谷底时，结果可能依赖于函数形态和算法实现）。\n",
    "### 用法\n",
    "    x_min = fminbound(func, x1, x2, args=(), xtol=1e-10, rtol=1e-10, maxfun=500, full_output=0, disp=0)\n",
    "#### 关键参数解释\n",
    "    func (必需):\n",
    "        要最小化的目标函数。它必须是一个接受单个标量 x 作为输入，并返回一个标量 f(x) 的 Python 函数。\n",
    "    x1, x2 (必需):\n",
    "        搜索区间的下限和上限。算法会在 [x1, x2] 区间内寻找最小值。x1 必须小于 x2。\n",
    "    args (可选):\n",
    "        一个元组，包含需要传递给 func 函数的额外参数。例如，如果你的函数是 f(x, a, b)，那么你可以设置 args=(a_value, b_value)。\n",
    "    xtol (可选, 默认 1e-10):\n",
    "        x 值的绝对容差。当区间长度小于 xtol 时，算法认为已经收敛。\n",
    "    rtol (可选, 默认 1e-10):\n",
    "        x 值的相对容差。当区间长度小于 rtol * |x| 时，算法认为已经收敛。算法会在 xtol 和 rtol 中取更严格的那个作为终止条件。\n",
    "    maxfun (可选, 默认 500):\n",
    "        允许对目标函数 func 的最大调用次数。达到此次数后，算法将停止。\n",
    "    full_output (可选, 默认 0):\n",
    "        控制返回值的详细程度。\n",
    "        如果 full_output=0 (默认)，函数只返回找到的最小值点 x_min。\n",
    "        如果 full_output=1，函数返回一个元组 (x_min, f_min, ierr, numfun)，其中：\n",
    "            x_min: 最小值点。\n",
    "            f_min: 在 x_min 处的函数值。\n",
    "            ierr: 整数错误代码 (0 表示成功，1 表示达到 maxfun)。\n",
    "            numfun: 实际调用目标函数的次数。\n",
    "    disp (可选, 默认 0):\n",
    "        控制是否打印消息。\n",
    "            disp=0: 不打印。\n",
    "            disp=1: 如果算法因达到 maxfun 而终止，则打印一条消息。\n",
    "            disp=2: 不管是否成功，都打印收敛消息。\n",
    "#### 返回值\n",
    "    当 full_output=0 (默认) 时，返回一个浮点数 x_min，即找到的最小值点。\n",
    "    当 full_output=1 时，返回一个包含 (x_min, f_min, ierr, numfun) 的元组。\n",
    "例一：求函数f(x) = e^xcos(2x)在区间[0,3]上的极小点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "826fbb35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "极小值点为:1.8026199149262752, 极小值为:-5.425165227463772\n"
     ]
    }
   ],
   "source": [
    "from numpy import exp, cos\n",
    "from scipy.optimize import fminbound\n",
    "\n",
    "f = lambda x: exp(x)*cos(2*x)\n",
    "x0 = fminbound(f,0,3)\n",
    "print('极小值点为:{}, 极小值为:{}'.format(x0, f(x0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ac40f4",
   "metadata": {},
   "source": [
    "scipy.optimize.fmin() 是 SciPy 优化模块 (scipy.optimize) 中的一个函数，用于使用 Nelder-Mead 单纯形算法来寻找多变量函数的最小值。\n",
    "\n",
    "它是一个经典的、不需要目标函数梯度（导数）信息的直接搜索优化方法。这使得它适用于优化那些不可导、不光滑或者计算梯度比较困难的函数。\n",
    "#### 核心算法：Nelder-Mead 单纯形法\n",
    "    单纯形 (Simplex)：在 n 维空间中，由 n+1 个点构成的几何图形。例如，2D 中是三角形，3D 中是四面体。\n",
    "#### 基本思想：\n",
    "    算法维护一个包含 n+1 个顶点的单纯形。通过一系列操作（反射 (Reflection)、扩张 (Expansion)、收缩 (Contraction) 和 缩边/压缩 (Shrinkage)）来更新这个单纯形：\n",
    "        找到单纯形中函数值最差的点（最“高”的点）。\n",
    "        尝试通过反射、扩张等方式，用一个函数值更优（更“低”）的新点来替换它。\n",
    "        如果成功，单纯形就向更优的区域移动。\n",
    "        如果失败，则通过收缩或压缩来缩小单纯形，进行更精细的局部搜索。\n",
    "目标：通过不断调整单纯形的形状和位置，使其最终“爬”到函数的（局部）最小值点附近并收缩到该点。\n",
    "#### 基本用法\n",
    "    xopt = fmin(func, x0, args=(), xtol=0.0001, ftol=0.0001, maxiter=None, maxfun=None, full_output=0, disp=1, retall=0, callback=None, initial_simplex=None)\n",
    "#### 关键参数解释\n",
    "    func (必需):\n",
    "        要最小化的目标函数。它必须接受一个包含 n 个元素的数组（x）作为输入，并返回一个标量（函数值 f(x)）。\n",
    "    x0 (必需):\n",
    "        初始猜测值，一个 n 维数组，表示搜索的起点。Nelder-Mead 算法会从这个点开始构建初始单纯形。\n",
    "    args (可选):\n",
    "        一个元组，包含需要传递给 func 函数的额外参数。\n",
    "    xtol (可选, 默认 1e-4):\n",
    "        x 值的终止容差。当单纯形中所有点在 x 空间上的变化小于 xtol 时，算法认为已经收敛。\n",
    "    ftol (可选, 默认 1e-4):\n",
    "        函数值 f(x) 的终止容差。当单纯形中所有点的函数值变化小于 ftol 时，算法认为已经收敛。\n",
    "    maxiter (可选):\n",
    "        最大迭代次数。如果未指定，则默认为 N*200，其中 N 是变量的个数。\n",
    "    maxfun (可选):\n",
    "        允许对目标函数 func 的最大调用次数。如果未指定，则默认为 N*200。\n",
    "    full_output (可选, 默认 0):\n",
    "        控制返回值的详细程度。\n",
    "            full_output=0 (默认): 只返回找到的最优解 xopt。\n",
    "            full_output=1: 返回一个元组 (xopt, {fopt, iter, funcalls, warnflag, [allvecs]})，其中包含：\n",
    "                fopt: 最优解处的函数值。\n",
    "                iter: 执行的迭代次数。\n",
    "                funcalls: 目标函数被调用的总次数。\n",
    "                warnflag: 警告标志 (0=成功, 1=达到 maxiter, 2=达到 maxfun)。\n",
    "                allvecs (仅当 retall=1 时包含): 所有迭代中单纯形顶点的列表。\n",
    "    disp (可选, 默认 1):\n",
    "        布尔值，是否打印收敛消息。\n",
    "    retall (可选, 默认 0):\n",
    "        布尔值，是否返回所有迭代中的单纯形顶点 (allvecs)。只有当 full_output=1 时才有效。\n",
    "    callback (可选):\n",
    "        一个函数，会在每次迭代后被调用，并将当前的最优解 xk 作为参数传入。可用于监控优化过程。\n",
    "    initial_simplex (可选):\n",
    "        一个形状为 (n+1, n) 的数组，允许你手动指定初始单纯形的 n+1 个顶点。如果提供，x0 参数将被忽略。\n",
    "#### 返回值\n",
    "    当 full_output=0 (默认) 时，返回一个数组 xopt，即找到的最优解。\n",
    "    当 full_output=1 时，返回一个元组，包含 xopt 和一个字典（或元组），其中包含 fopt, iter, funcalls, warnflag 等信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dbd43215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.000000\n",
      "         Iterations: 94\n",
      "         Function evaluations: 175\n",
      "找到的最优解: [1.00000007 1.00000011]\n",
      "最优解: [1.00000439 1.00001064]\n",
      "最优函数值: 3.6861769151759075e-10\n",
      "迭代次数: 79\n",
      "函数调用次数: 146\n",
      "警告标志: 0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import fmin\n",
    "\n",
    "# 定义一个二维函数 (Rosenbrock 函数，最小值在 (1, 1))\n",
    "def rosenbrock(x):\n",
    "    return (1 - x[0])**2 + 100 * (x[1] - x[0]**2)**2\n",
    "# 初始猜测\n",
    "x0 = [0, 0]\n",
    "# 使用 fmin 进行优化\n",
    "xopt = fmin(rosenbrock, x0, xtol=1e-6, ftol=1e-6, maxiter=1000, disp=True)\n",
    "print(f\"找到的最优解: {xopt}\")\n",
    "# 使用 full_output 获取更多信息\n",
    "xopt, fopt, iter_count, func_calls, warn_flag = fmin(rosenbrock, x0, full_output=1, disp=False)\n",
    "print(f\"最优解: {xopt}\")\n",
    "print(f\"最优函数值: {fopt}\")\n",
    "print(f\"迭代次数: {iter_count}\")\n",
    "print(f\"函数调用次数: {func_calls}\")\n",
    "print(f\"警告标志: {warn_flag}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023f0fce",
   "metadata": {},
   "source": [
    "例二：求函数f(x) = e^xcos(2x)在0附近的极小点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ab2a99d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: -0.234443\n",
      "         Iterations: 26\n",
      "         Function evaluations: 52\n",
      "极小点为:[-1.339], 极小值为:[-0.23444265]\n"
     ]
    }
   ],
   "source": [
    "from numpy import exp, cos\n",
    "from scipy.optimize import fmin\n",
    "\n",
    "f = lambda x: exp(x)*cos(2*x)\n",
    "x0 = fmin(f,0)\n",
    "print('极小点为:{}, 极小值为:{}'.format(x0,f(x0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2b32ac",
   "metadata": {},
   "source": [
    "例三：求函数f(x) = 100(x2-x1^2)^2+(1-x1)^2的极小值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5f93b3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "极小点为:[0.99999565 0.99999129], 极小值为:1.8932783589357527e-11\n"
     ]
    }
   ],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "f = lambda x: 100*(x[1]-x[0]**2)**2+(1-x[0])**2\n",
    "x0 = minimize(f, [2.0, 2.0])\n",
    "print('极小点为:{}, 极小值为:{}'.format(x0.x, x0.fun))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
