{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "024680db",
   "metadata": {},
   "source": [
    "# Logistic回归\n",
    "其分类的结果输出是有限的离散值，基本思想是在空间中构造一个合理的超平面，把空间区域划分为两个子空间，每一种类别数据都在平面的某一侧。\n",
    "Logistic回归通过使用逻辑函数（也称为sigmoid函数）来估计概率，从而预测分类结果。其核心思想是根据输入特征计算出一个概率值，然后根据这个概率值进行分类决策。\n",
    "    Logistic回归使用sigmoid函数将线性回归的输出映射到(0,1)区间：\n",
    "    σ(z) = 1 / (1 + e^(-z))\n",
    "        其中z是输入特征的线性组合：\n",
    "        z = β₀ + β₁x₁ + β₂x₂ + ... + βₙxₙ\n",
    "# Logistic回归模型可以表示为：\n",
    "P(Y=1|X) = 1 / (1 + e^(-(β₀ + β₁x₁ + ... + βₙxₙ)))\n",
    "其中：\n",
    "    P(Y=1|X)是在给定特征X下Y=1的概率\n",
    "    β₀是截距项\n",
    "    β₁到βₙ是各特征的系数\n",
    "# 参数估计\n",
    "Logistic回归通常使用极大似然估计(MLE)来估计参数。目标是找到一组参数，使得观测数据的似然性最大化。\n",
    "在Logistic回归中，MLE用于估计回归系数β。对数似然函数为：\n",
    "ℓ(β) = ∑ [yᵢ log p(xᵢ) + (1-yᵢ) log(1-p(xᵢ))]\n",
    "其中p(xᵢ) = 1/(1+exp(-βᵀxᵢ))。通过数值优化方法(如牛顿法、梯度下降等)最大化这个对数似然函数来估计β。\n",
    "\n",
    "# 分组数据情形下参数的最小二乘估计\n",
    "在对因变量进行的n次观测yi，如果在相同的x(i)处进行了多次重复观测，这种结构的数据称为分组数据。可用样本比例对概率p进行估计，对应的x(i)的概率估计值记作(估计值)pi,\n",
    "yi* = ln(pi/1-pi)----->yi* = β₀+Σβᵢxᵢj\n",
    "\n",
    "# 可以通过statsmodels库函数求解"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ea04b93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.924\n",
      "Model:                            OLS   Adj. R-squared:                  0.913\n",
      "Method:                 Least Squares   F-statistic:                     85.42\n",
      "Date:                Sun, 27 Jul 2025   Prob (F-statistic):           3.59e-05\n",
      "Time:                        16:13:05   Log-Likelihood:                 6.6829\n",
      "No. Observations:                   9   AIC:                            -9.366\n",
      "Df Residuals:                       7   BIC:                            -8.971\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -0.8863      0.102     -8.653      0.000      -1.128      -0.644\n",
      "x1             0.1558      0.017      9.242      0.000       0.116       0.196\n",
      "==============================================================================\n",
      "Omnibus:                        5.866   Durbin-Watson:                   2.311\n",
      "Prob(Omnibus):                  0.053   Jarque-Bera (JB):                1.209\n",
      "Skew:                          -0.036   Prob(JB):                        0.546\n",
      "Kurtosis:                       1.206   Cond. No.                         14.6\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "所求的概率p0=0.6262\n"
     ]
    }
   ],
   "source": [
    "#例一\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "\n",
    "a = np.loadtxt('Pdata12_7_1.txt')\n",
    "x = a[:,0]\n",
    "pi = a[:,2]/a[:,1]\n",
    "X = sm.add_constant(x)\n",
    "yi = np.log(pi/(1-pi))  #yi* = ln(pi/1-pi)\n",
    "md = sm.OLS(yi,X).fit()  #构建并拟合模型\n",
    "print(md.summary())\n",
    "b = md.params  #提出所有的回归系数\n",
    "# print(b)\n",
    "p0 = 1/(1+np.exp(-np.dot(b, [1,9])))  #构造Logistic回归方程\n",
    "\"\"\"\n",
    "np.dot(b, [1, 9])\n",
    "b 是 Logistic 回归的系数向量，假设 b = [β₀, β₁]（截距 + 斜率）。\n",
    "[1, 9] 是特征向量，其中 1 是截距项（x₀），9 是某个特征值（x₁）。\n",
    "np.dot(b, [1, 9]) 计算的是 线性组合：z=β0·1+β1⋅9\n",
    "\"\"\"\n",
    "print('所求的概率p0=%.4f' %p0)\n",
    "np.savetxt('Pdata12_7_2.txt',b)  #把回归系数保存到文本文件"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e47d06",
   "metadata": {},
   "source": [
    "# Logistic模型的应用\n",
    "1. 在流行病学中，经常需要研究某一疾病发生与不发生的可能性大小，如一个人得流行性感冒相对于不得流行性感冒的可能性是多少，对此常用赔率来度量\n",
    "    (1) 赔率的定义\n",
    "    一个随机事件A发生的概率与其不发生的概率之比值称为事件A的赔率，记为odds(A),即odds(A) = P(A)/(1-P(A)).\n",
    "        用赔率可以很好的度量一些经济现象发生与否的可能性大小\n",
    "    (2) 赔率比的定义\n",
    "    随机事件A的赔率与随机事件B的赔率之比值称为事件A对事件B的赔率比，记为OR(A,B) = odds(A) / odds(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92e9da4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odds_9=1.6752\n",
      "odds_9vs8=1.1686\n"
     ]
    }
   ],
   "source": [
    "#例一续\n",
    "import numpy as np\n",
    "\n",
    "b = np.loadtxt('Pdata12_7_2.txt')\n",
    "odds_9 = np.exp(np.dot(b,[1,9]))  #$记算x=9时的赔率\n",
    "odds_9vs8 = np.exp(np.dot([1,9],b))/np.exp(np.dot(b,[1,8]))\n",
    "print('odds_9=%.4f\\nodds_9vs8=%.4f' %(odds_9, odds_9vs8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd342c60",
   "metadata": {},
   "source": [
    "# 通过statsmodels库函数求解并进行预测"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38edcb71",
   "metadata": {},
   "source": [
    "1. 牛顿法原理\n",
    "牛顿法通过使用二阶导数信息（Hessian矩阵）来加速收敛。对于函数f(θ)，其迭代公式为：\n",
    "    θ^(t+1) = θ^(t) - [H(θ^(t))]⁻¹ ∇f(θ^(t))\n",
    "其中：\n",
    "    ∇f(θ)是梯度向量（一阶导数）\n",
    "    H(θ)是Hessian矩阵（二阶导数）\n",
    "\n",
    "在Logistic回归中的应用\n",
    "对于Logistic回归的对数似然函数ℓ(β)，牛顿法的迭代步骤为：\n",
    "β^(t+1) = β^(t) + (X'WX)⁻¹ X'(y - p)\n",
    "其中：\n",
    "    X是设计矩阵\n",
    "    W是对角权重矩阵，Wᵢᵢ = pᵢ(1-pᵢ)\n",
    "    p是预测概率向量\n",
    "    y是响应变量向量\n",
    "\n",
    "2. bfgs法\n",
    "BFGS（Broyden-Fletcher-Goldfarb-Shanno）是最流行的拟牛顿优化算法之一，特别适用于Logistic回归等光滑非线性优化问题。\n",
    "    1. BFGS算法原理\n",
    "BFGS是一种拟牛顿法，通过迭代近似Hessian矩阵（而不是直接计算），具有以下特点：\n",
    "    不需要计算二阶导数，仅使用一阶导数（梯度）信息\n",
    "    保持Hessian近似矩阵的正定性，确保下降方向\n",
    "    具有超线性收敛速度\n",
    "    2. 迭代公式\n",
    "BFGS的更新步骤如下：\n",
    "    计算搜索方向：dₖ = -Bₖ⁻¹∇f(θₖ)\n",
    "    线搜索确定步长αₖ\n",
    "    更新参数：θₖ₊₁ = θₖ + αₖdₖ\n",
    "    更新Hessian近似矩阵Bₖ₊₁\n",
    "其中Hessian近似矩阵B的BFGS更新公式为：\n",
    "    Bₖ₊₁ = Bₖ + (yₖyₖᵀ)/(yₖᵀsₖ) - (BₖsₖsₖᵀBₖ)/(sₖᵀBₖsₖ)\n",
    "    其中sₖ = θₖ₊₁ - θₖ，yₖ = ∇f(θₖ₊₁) - ∇f(θₖ)\n",
    "在Logistic回归中的应用\n",
    "    优势\n",
    "        内存效率：对于高维问题，可以使用有限内存版本(L-BFGS)\n",
    "        数值稳定性：不需要直接计算Hessian矩阵\n",
    "        适应性强：适用于各种规模的数据集\n",
    "    实现步骤\n",
    "        定义对数似然函数ℓ(β)及其梯度\n",
    "        初始化参数β₀和Hessian近似矩阵B₀（通常取单位矩阵）\n",
    "        迭代更新直至收敛"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "02fa41be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.000002\n",
      "         Iterations: 18\n",
      "         Function evaluations: 21\n",
      "         Gradient evaluations: 21\n",
      "[-0.34967435  3.22899624  2.23724381] \n",
      "---------------\n",
      "\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   No. Observations:                   20\n",
      "Model:                          Logit   Df Residuals:                       17\n",
      "Method:                           MLE   Df Model:                            2\n",
      "Date:                Sun, 27 Jul 2025   Pseudo R-squ.:                   1.000\n",
      "Time:                        18:44:22   Log-Likelihood:            -4.6356e-05\n",
      "converged:                       True   LL-Null:                       -13.863\n",
      "Covariance Type:            nonrobust   LLR p-value:                 9.537e-07\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "x1            -0.3497    187.842     -0.002      0.999    -368.514     367.815\n",
      "x2             3.2290   1520.250      0.002      0.998   -2976.406    2982.864\n",
      "x3             2.2372   5400.730      0.000      1.000   -1.06e+04    1.06e+04\n",
      "==============================================================================\n",
      "\n",
      "Complete Separation: The results show that there iscomplete separation or perfect prediction.\n",
      "In this case the Maximum Likelihood Estimator does not exist and the parameters\n",
      "are not identified.\n",
      "[4.39453842e-17 1.00000000e+00]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "\n",
    "a = np.loadtxt('Pdata12_9.txt')\n",
    "n = a.shape[1]  #提取矩阵的列数\n",
    "x = a[:,:n-1]\n",
    "y = a[:,n-1]\n",
    "md = sm.Logit(y,x)\n",
    "md = md.fit(method='bfgs')  #这里必须使用bfgs法，使用默认牛顿法会出错\n",
    "\"\"\"\n",
    "1. 牛顿法需要计算Hessian矩阵(二阶导数矩阵)的逆。如果：\n",
    "    数据存在完全分离(perfect separation)(某些特征能完美预测分类)\n",
    "    存在高度相关的特征(导致矩阵奇异)\n",
    "    样本量过少或特征过多\n",
    "Hessian矩阵可能无法求逆,导致报错\n",
    "2. 初始值敏感\n",
    "    牛顿法对初始参数值敏感，如果初始值离最优解太远，可能无法收敛。\n",
    "\n",
    "BFGS是一种拟牛顿法\n",
    "    它通过近似Hessian矩阵(而不是直接计算)，避免了矩阵求逆的数值不稳定性。\n",
    "    即使Hessian矩阵不可逆,BFGS仍可能收敛。\n",
    "更鲁棒的优化\n",
    "    BFGS使用一阶导数(梯度)信息,对初始值的依赖性较低。\n",
    "    适合处理非凸优化问题（如逻辑回归的对数似然函数）。\n",
    "\"\"\"\n",
    "print(md.params, '\\n---------------\\n')\n",
    "print(md.summary())\n",
    "#求预测值\n",
    "pre = md.predict([[-49.2,-17.2,0.3], [40.6,26.4,1.8]])\n",
    "print(pre)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82b169e",
   "metadata": {},
   "source": [
    "# 使用sklearn 库函数进行求解并进行预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a344bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.39060121] [[-0.05073412  0.67071015  0.10511888]]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "[0. 1.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "a = np.loadtxt('Pdata12_9.txt')\n",
    "n = a.shape[1]\n",
    "x = a[:,:n-1]\n",
    "y = a[:,n-1]\n",
    "md = LogisticRegression(solver='lbfgs').fit(x,y)\n",
    "print(md.intercept_, md.coef_)  #输出截距项和斜率\n",
    "print(md.predict(x))  #检验预测模型\n",
    "print(md.predict([[-49.2,-17.2,0.3], [40.6,26.4,1.8]]))  #求预测值"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
